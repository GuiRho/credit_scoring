services:
  mlflow:
    image: ghcr.io/mlflow/mlflow:latest
    ports:
      - "5000:5000"
    volumes:
      - ./mlruns:/mlruns
      - ./mlflow_artifact:/mlflow_artifact
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - MLFLOW_TRACKING_SERVER_HOST=0.0.0.0
      - MLFLOW_TRACKING_SERVER_PORT=5000
    command: mlflow ui --backend-store-uri file:///mlruns --artifacts-destination file:///mlflow_artifact --host 0.0.0.0 --port 5000
    healthcheck:
      test: ["CMD", "python", "-c", "import socket; s = socket.socket(socket.AF_INET, socket.SOCK_STREAM); s.connect((\"localhost\", 5000)); s.close()"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - credit_scoring-network
    restart: always

  model-api:
    build: ./serve_model
    ports:
      - "8000:8000"
    networks:
      - credit_scoring-network
    volumes:
      - ./model_artifacts:/app/model_artifacts   # <-- mount the pre-trained model here
    healthcheck:
      test: ["CMD", "python", "-c", "import requests, sys; r = requests.get('http://localhost:8000/health'); sys.exit(0 if r.status_code==200 else 1)"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: always

  streamlit-app:
    build: ./UX
    ports:
      - "8501:8501"
    environment:
      - API_URL=http://model-api:8000/predict
    depends_on:
      model-api:
        condition: service_healthy
    networks:
      - credit_scoring-network
    restart: always

networks:
  credit_scoring-network:
    driver: bridge
