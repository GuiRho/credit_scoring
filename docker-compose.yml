services:
  mlflow:
    image: ghcr.io/mlflow/mlflow:latest
    ports:
      - "5000:5000"
    volumes:
      - ./mlruns:/mlruns
      - ./mlflow_artifact:/mlflow_artifact
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - MLFLOW_TRACKING_SERVER_HOST=0.0.0.0
      - MLFLOW_TRACKING_SERVER_PORT=5000
    command: mlflow ui --backend-store-uri file:///mlruns --artifacts-destination file:///mlflow_artifact --host 0.0.0.0 --port 5000
    healthcheck:
      test: ["CMD", "python", "-c", "import socket; s = socket.socket(socket.AF_INET, socket.SOCK_STREAM); s.connect((\"localhost\", 5000)); s.close()"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - gtrack-network

  model-api:
    build: ./serve_model
    ports:
      - "8000:8000"
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5000
    depends_on:
      mlflow:
        condition: service_healthy
    networks:
      - gtrack-network

  streamlit-app:
    build: ./UX
    ports:
      - "8501:8501"
    environment:
      - API_URL=http://model-api:8000/predict
      - MLFLOW_TRACKING_URI=http://mlflow:5000
    depends_on:
      model-api:
        condition: service_started
    networks:
      - gtrack-network

  ai-input-1:
    build: ./ai_input_1
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5000
    depends_on:
      mlflow:
        condition: service_healthy
    networks:
      - gtrack-network
    volumes:
      - C:/Users/gui/Documents/OpenClassrooms/Projet 7/Enonce/:/app/data_source
    command: python ai_input_1.py

networks:
  gtrack-network:
    driver: bridge