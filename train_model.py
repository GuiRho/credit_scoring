import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, make_scorer
import joblib
import mlflow
import os

# --- Configuration ---
MLFLOW_TRACKING_URI = "http://127.0.0.1:5000"
REGISTERED_MODEL_NAME = "CreditScoringRF"

# Best hyperparameters from Optuna
BEST_PARAMS = {
    'n_estimators': 470,
    'max_depth': 22,
    'min_samples_split': 15,
    'min_samples_leaf': 6,
    'max_features': 'sqrt',
    'criterion': 'entropy',
    'class_weight': None
}

def custom_confusion_score(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    if cm.shape == (2, 2):
        tn, fp, fn, tp = cm.ravel()
        score = tp - fp - 8 * fn + 4 * tn
    else:
        score = -10000
    return score

custom_scorer = make_scorer(custom_confusion_score, greater_is_better=True)

def main():
    """Train and register the model."""
    # --- MLflow Setup ---
    os.environ["MLFLOW_TRACKING_URI"] = MLFLOW_TRACKING_URI
    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
    mlflow.set_experiment("Credit Scoring Model Training")

    with mlflow.start_run(run_name="Final Model Training") as run:
        # --- Load Preprocessed Data ---
        # The df_global.parquet is now generated by preprocess_data.py and saved in the project root
        df_global_path = "c:\Users\gui\Documents\credit_scoring\df_global.parquet"
        df_global = pd.read_parquet(df_global_path)
        print(f"Loaded preprocessed data from {df_global_path}. Shape: {df_global.shape}")

        # Assuming the 'TARGET' column is present and is the target variable
        if 'TARGET' not in df_global.columns:
            raise ValueError("TARGET column not found in the preprocessed dataframe.")

        X = df_global.drop('TARGET', axis=1)
        y = df_global['TARGET']

        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        # --- Train Model ---
        model = RandomForestClassifier(**BEST_PARAMS, random_state=42, n_jobs=-1)
        model.fit(X_train_scaled, y_train)

        # --- Evaluate Model ---
        y_pred = model.predict(X_test_scaled)
        score = custom_confusion_score(y_test, y_pred)
        mlflow.log_metric("custom_score", score)
        mlflow.log_params(BEST_PARAMS)

        # --- Save Model, Scaler, and Feature Columns ---
        joblib.dump(model, "model.joblib")
        joblib.dump(scaler, "scaler.joblib")
        joblib.dump(X.columns.tolist(), "model_features.joblib") # Save feature columns
        mlflow.log_artifact("model.joblib")
        mlflow.log_artifact("scaler.joblib")
        mlflow.log_artifact("model_features.joblib") # Log feature columns as artifact

        # --- Register Model ---
        mlflow.register_model(
            f"runs:/{run.info.run_id}/model.joblib",
            REGISTERED_MODEL_NAME
        )

if __name__ == "__main__":
    main()